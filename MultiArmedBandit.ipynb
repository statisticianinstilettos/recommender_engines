{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit \n",
    "\n",
    "Multi-Armed Bandits are a type of Reinforcement Learning algorithmn that is used to learn the best actions to maximize an expected gain. The algorithm learns about the distribution of the gain for the different actions over time by using an exploration(aquire new knowledge and exploitation(optimize the decisions based on existing knowledge) tradeoff stragegy. The agent attempts to balance these competing tasks in order to maximize the total gain over time.\n",
    "\n",
    "\n",
    "## Epsilon Greedy\n",
    "$\\epsilon$ = probability of exploration.  This is the is the percentage of exploration actions the agent will take. For example if $\\epsilon$=0.05, then 5% of actions will be exploration, and 95% will be exploitation. \n",
    "\n",
    "Epsilon Greedy allows the agent to explore all actions. The agent learns the best actions by updating the mean of each bandit after each action is taken. \n",
    "\n",
    "After the optimal actions are identified, the epsilon greedy algorithm will still explore suboptimal actions at the rate of $\\epsilon$.\n",
    "\n",
    "## Optimistic Initialization\n",
    "Optimistic intialization is an alternative to the epsilon greedy algorithm. It forces the agent to explore all actions by setting the inital mean of all the bandits as very high, ie an upper limit. Means should be selected to be much higher than what the true mean could be. This forces the agent to explore all the bandits, and the bandit's means will decrease quickly. \n",
    "\n",
    "## UCB1\n",
    "Similar to optimistic initalization, UCB1 initializes the mean of the bandits using an upper bound. This upper bound is based on confidence bounds and is determined by the Hoeffdingâ€™s Inequality.\n",
    "$$P\\{|\\bar{X}-\\mu| \\geq \\epsilon\\} \\leq 2e^{-2\\epsilon^2N}$$\n",
    "which rearranges to an upper bound on the mean for the jth bandit\n",
    "$$X_{UCB-j}=\\bar{X_j}+\\sqrt{2\\frac{lnN}{N_j}}$$\n",
    "Where N is the number of times all bandits have been played. \n",
    "\n",
    "This allows the initialized mean values of the bandits to shrink as the agent tries each bandit, and becomes more confident in our estimate of the bandits true mean. \n",
    "\n",
    "## Bayesian Multi-Armed Bandits\n",
    "\n",
    "In the bayesian multiarmed bandit, the an upper limit is used to initalize the means of the bandits. The upper limit is a upper confidence bound calculated using a distribution of the mean given the observed data, $p(\\mu|X)$. This is the posterior of $\\mu$.\n",
    "\n",
    "We use bayes rule to calculate the posterior. \n",
    "\n",
    "$$p(\\mu|X)=p(X|\\mu)p(\\mu)$$\n",
    "\n",
    "We assume X follow a normal $X \\sim Normal(\\mu, \\sigma^2/N)$.\n",
    "\n",
    "And we put a prior on the mean $\\mu \\sim Beta(a, b)$\n",
    "\n",
    "The upper limit is selected as the max of the samples from each of the bandits. The distribution is fat )has large confidence intervals) when few samples have been observed, and becomes skinnier as we approximate the true mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed\n",
      "[nltk_data]     (_ssl.c:852)>\n"
     ]
    }
   ],
   "source": [
    "from src.reinforcement_learning import GreedyBandit, OptimisticBandit, UCB1Bandit, BayesianBandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2943410919080815\n",
      "0.12452945435739586\n",
      "0.5058750709062644\n"
     ]
    }
   ],
   "source": [
    "bandits = GreedyBandit.perform_epsilon_greedy(bandit_true_means=[0.01, 0.05, 0.5], N=1000, epsilon=0.1)\n",
    "for b in bandits: print(b.mean)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.05\n",
      "10.01422818586346\n"
     ]
    }
   ],
   "source": [
    "bandits = OptimisticBandit.perform_optimistic_initialization(bandit_true_means=[0.01, 0.05, 0.5], N=1000, upper_limit=10)\n",
    "for b in bandits: print(b.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010878899532350206\n",
      "-0.3471834886352026\n",
      "0.4943624580534103\n"
     ]
    }
   ],
   "source": [
    "bandits = UCB1Bandit.perform_ucb1(bandit_true_means=[0.01, 0.05, 0.5], N=1000, upper_limit=10)\n",
    "for b in bandits: print(b.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0008822362175584519\n",
      "-0.0025015484072800163\n",
      "0.5244307547234343\n"
     ]
    }
   ],
   "source": [
    "bandits = BayesianBandit.perform_bayesian_bandits(bandit_true_means=[0.01, 0.05, 0.5], N=1000)\n",
    "for b in bandits: print(b.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender_engines",
   "language": "python",
   "name": "recommender_engines"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
